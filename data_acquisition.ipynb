{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182e44ae-bede-425b-9c9f-3964685d6aa7",
   "metadata": {},
   "source": [
    "## Data Acquisition Exercises\n",
    "\n",
    "\n",
    "By the end of this exercise, you should have a file named acquire.py that contains the specified functions. If you wish, you may break your work into separate files for each website (e.g. acquire_codeup_blog.py and acquire_news_articles.py), but the end function should be present in acquire.py (that is, acquire.py should import get_blog_articles from the acquire_codeup_blog module.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12739d7-1c03-45f3-819d-4962ef28193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea804ce-48fa-43ba-bb15-1b12d008c850",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Codeup Blog Articles\n",
    "\n",
    "Visit Codeup's Blog (https://codeup.edu/blog/) and record the urls for at least 5 distinct blog posts. For each post, you should scrape at least the post's title and content.\n",
    "\n",
    "Encapsulate your work in a function named get_blog_articles that will return a list of dictionaries, with each dictionary representing one article. The shape of each dictionary should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c3dd51-d86d-4d88-9dfd-6a0c040cd510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\n",
    "#    'title': 'the title of the article',\n",
    "#    'content': 'the full text content of the article'\n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ee19e-75ca-4b85-83d0-1b464589555e",
   "metadata": {},
   "source": [
    "Plus any additional properties you think might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74b1f1b0-3bae-49c9-bb39-826305ced566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_content(url):\n",
    "    response = requests.get(url, headers={'User-Agent': 'Codeup Data Science'})\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e9a8c28-cdd2-4d91-8f72-84c5881d3b89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pg1 = get_page_content('https://codeup.edu/blog/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5192ea0-5c2c-4f35-a631-4c46108fce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(page):\n",
    "    links = page.find_all(\"h2\")\n",
    "    new_links = []\n",
    "    for article in links:\n",
    "        if article.find(\"a\"):\n",
    "            new_links.append(article.find(\"a\").get(\"href\"))\n",
    "    return new_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dadd737-8659-4757-8ccc-cacc0ae4cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_pg1 = extract_links(pg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d077c0ba-0a2c-4359-b85a-f5891b3d4f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://codeup.edu/featured/apida-heritage-month/',\n",
       " 'https://codeup.edu/featured/women-in-tech-panelist-spotlight/',\n",
       " 'https://codeup.edu/featured/women-in-tech-rachel-robbins-mayhill/',\n",
       " 'https://codeup.edu/codeup-news/women-in-tech-panelist-spotlight-sarah-mellor/',\n",
       " 'https://codeup.edu/events/women-in-tech-madeleine/',\n",
       " 'https://codeup.edu/codeup-news/panelist-spotlight-4/']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_pg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fffd4e2a-d24e-43fc-87f9-b3239076ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(link):\n",
    "    page_soup = get_page_content(link)\n",
    "    header = page_soup.find(\"h1\").get_text()\n",
    "    content = page_soup.select(\".entry-content\")[0].find_all(\"p\")\n",
    "    clean = []\n",
    "    for p in content:\n",
    "        clean.append(p.get_text())\n",
    "    clean = ' '.join(clean)\n",
    "    page_content = {'title': header,\n",
    "                    'content': clean}\n",
    "    return page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be0ba659-f2bd-4944-89bb-e3a33dab4239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Spotlight on APIDA Voices: Celebrating Heritage and Inspiring Change ft. Arbeena Thapa',\n",
       " 'content': 'May is traditionally known as Asian American and Pacific Islander (AAPI) Heritage Month. This month we celebrate the history and contributions made possible by our AAPI friends, family, and community. We also examine our level of support and seek opportunities to better understand the AAPI community.  In an effort to address real concerns and experiences, we sat down with Arbeena Thapa, one of Codeup’s Financial Aid and Enrollment Managers. Arbeena identifies as Nepali American and Desi. Arbeena’s parents immigrated to Texas in 1988 for better employment and educational opportunities. Arbeena’s older sister was five when they made the move to the US. Arbeena was born later, becoming the first in her family to be a US citizen. At Codeup we take our efforts at inclusivity very seriously. After speaking with Arbeena, we were taught that the term AAPI excludes Desi-American individuals. Hence, we will now use the term Asian Pacific Islander Desi American (APIDA). Here is how the rest of our conversation with Arbeena went! How do you celebrate or connect with your heritage and cultural traditions? “I celebrate Nepal’s version of Christmas or Dashain. This is a nine-day celebration also known as Dussehra. I grew up as Hindu and I identify as Hindu, this is a very large part of my heritage. “ “Other ways I connect with my culture include sharing food! Momos are South Asian Dumplings and they’re my favorite to make and share.” “On my Asian American side, I am an advocate of immigrant justice and erasure within APIDA social or political movements. I participate in events to embrace my identity such as immigrant justice advocacy because I come from a mixed-status family. I’ve always been in a community with undocumented Asian immigrants. .” What are some of the challenges you have faced as an APIDA individual, personally or professionally? “I often struggle with being gendered as compliant or a pushover. Professionally, I am often stereotyped as meek, so I’ve been overlooked for leadership roles. We are seen as perpetually foreign; people tend to other us in that way, yet put us on a pedestal for what a model minority looks like. This has made me hesitant to share my heritage in the past because these assumptions get mapped onto me. ” Can you describe some common barriers of entry that APIDA individuals, specifically women may face when trying to enter or advance in the workplace? “Being overlooked for leadership. In the past, I have not been viewed as a leader. People sometimes have preconceived stereotypes of Asian women not being able to be bold, or being vocal can be mistaken for being too emotional. “ How do you believe microaggressions impact APIDA individuals in the workplace? Can you provide examples of such microaggressions? “Erasure is big. To me, only saying ‘Merry Christmas’ isn’t inclusive to other religions. People are often resistant to saying ‘Happy Holidays,’ but saying Merry Christmas excludes, and does not appreciate my heritage. “ “Often microaggressions are not micro at all. They typically are not aggressive racialized violence, but the term ‘micro’ minimizes impact.” “Some that I’ve heard are ‘What kind of Asian are you?’ or ‘Where are you from?’ This automatically makes me the ‘other’ and not seen as American. Even within the APIDA community, South Asians are overlooked as “Asian”.” How important is representation, specifically APIDA representation, in organizational leadership positions? “I want to say that it is important to have someone who looks like you in leadership roles, and it is, but those leaders may not share the same beliefs as you. Certain privileges such as wealth, resources, or lack of interaction with lower-socioeconomic-status Asian Americans may cause a difference in community politics. I do not think the bamboo ceiling is acceptable, but the company you work for plays a big part in your politics and belief alignment.” How do you feel about code-switching, and have you ever felt it necessary to code-switch? “I like sharing South Asian terms or connecting with others that have similar heritage and culture. A workplace that is welcoming to going into this sort of breakout is refreshing and makes space for us. However, having to code-switch could also mean a workplace that is not conducive and welcoming of other cultures. “ Finally, in your opinion, what long-term strategies can create lasting change in the workplace and ensure support, equality, and inclusion for APIDA individuals? “Prior to a career in financial aid, I did a lot of research related to the post-9/11 immigration of the South Asian diaspora. This background made me heavily rely on grassroots organizing. Hire the people that want to innovate, hire the changemakers, hire the button-pushers. Reduce reliance on whiteness as change. This will become natural for the organization and become organizational change. Change comes from us on the ground.” A huge thank you to Arbeena Thapa for sharing her experiences, and being vulnerable with us. Your words were inspiring and the opportunity to understand your perspective more has been valuable. We hope we can become better support for the APIDA community as we learn and grow on our journey of cultivating inclusive growth.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_content(links_pg1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab92948c-bbf4-47bd-ab67-8d76b29efe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    all_content = []\n",
    "    page = get_page_content(url)\n",
    "    links = extract_links(page)\n",
    "    for link in links:\n",
    "        all_content.append(get_content(link))\n",
    "    return all_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63035e68-95ed-4dec-a8a3-2e89b6fc3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_one = scrape('https://codeup.edu/blog/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6a0ca290-d145-4f1b-8800-c8af5f36f292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(page_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d48e7c5f-9fe1-44d6-87af-5558b4f1fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(url):\n",
    "    all_pages = []\n",
    "    all_pages.append(url)\n",
    "    while True:\n",
    "        page = get_page_content(all_pages[-1])\n",
    "        previous_page = page.select(\".alignleft\")[0].find(\"a\")\n",
    "        if previous_page == None:\n",
    "            break\n",
    "        previous_page_link = previous_page.get(\"href\")\n",
    "        all_pages.append(previous_page_link)\n",
    "    return all_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3076d559-0432-4f06-88f5-e32bb74244e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 done!\n",
      "Page 2 done!\n",
      "Page 3 done!\n",
      "Page 4 done!\n",
      "Page 5 done!\n",
      "Page 6 done!\n",
      "Page 7 done!\n",
      "Page 8 done!\n",
      "Page 9 done!\n",
      "Page 10 done!\n",
      "Page 11 done!\n",
      "Page 12 done!\n",
      "Page 13 done!\n",
      "Page 14 done!\n",
      "Page 15 done!\n",
      "Page 16 done!\n",
      "Page 17 done!\n",
      "Page 18 done!\n",
      "Page 19 done!\n",
      "Page 20 done!\n",
      "Page 21 done!\n",
      "Page 22 done!\n",
      "Page 23 done!\n",
      "Page 24 done!\n",
      "Page 25 done!\n",
      "Page 26 done!\n",
      "Page 27 done!\n",
      "Page 28 done!\n",
      "Page 29 done!\n",
      "Page 30 done!\n",
      "Page 31 done!\n",
      "Page 32 done!\n",
      "Page 33 done!\n",
      "Page 34 done!\n",
      "Page 35 done!\n",
      "Page 36 done!\n",
      "Page 37 done!\n",
      "Page 38 done!\n",
      "Page 39 done!\n",
      "Page 40 done!\n",
      "Page 41 done!\n",
      "Page 42 done!\n",
      "Page 43 done!\n",
      "Page 44 done!\n",
      "Page 45 done!\n"
     ]
    }
   ],
   "source": [
    "links = scrape_links('https://codeup.edu/blog/')\n",
    "everything = []\n",
    "i=1\n",
    "for link in links:\n",
    "    everything.append(scrape(link))\n",
    "    print(f'Page {i} done!')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "769895f2-88fc-4a99-982c-3c35b06ff02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208680ea-fc72-4573-a4d3-3347d24e974b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. News Articles\n",
    "\n",
    "We will now be scraping text data from inshorts (https://inshorts.com/en/read), a website that provides a brief overview of many different topics.\n",
    "\n",
    "Write a function that scrapes the news articles for the following topics:\n",
    "\n",
    "* Business\n",
    "* Sports\n",
    "* Technology\n",
    "* Entertainment\n",
    "\n",
    "The end product of this should be a function named get_news_articles that returns a list of dictionaries, where each dictionary has this shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11f15e-409a-4eb9-b4f6-2904f40f9b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\n",
    "#    'title': 'The article title',\n",
    "#    'content': 'The article content',\n",
    "#    'category': 'business' # for example\n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929c284-f16f-4aad-a626-565c672bc658",
   "metadata": {},
   "source": [
    "Hints:\n",
    "\n",
    "1. Start by inspecting the website in your browser. Figure out which elements will be useful.\n",
    "0. Start by creating a function that handles a single article and produces a dictionary like the one above.\n",
    "0. Next create a function that will find all the articles on a single page and call the function you created in the last step for every article on the page.\n",
    "0. Now create a function that will use the previous two functions to scrape the articles from all the pages that you need, and do any additional processing that needs to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8725cdc7-dfa9-4904-9b2d-78096235c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = ['business', 'politics', 'sports', 'technology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "27512260-ed91-442c-8b95-4d623f5d2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inshorts(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f881538a-f593-4c32-9387-d40142c3b010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_inshorts(cat):\n",
    "    page = get_inshorts(f'https://inshorts.com/en/read/{cat}')\n",
    "    headlines = page.find_all(itemprop='headline')\n",
    "    body = page.find_all(itemprop='articleBody')\n",
    "\n",
    "    clean_headlines = []\n",
    "    clean_body = []\n",
    "\n",
    "    for h,b in zip(headlines,body):\n",
    "        clean_headlines.append(h.get_text())\n",
    "        clean_body.append(b.get_text())\n",
    "        \n",
    "    all_data = []\n",
    "    for head,body in zip(clean_headlines, clean_body):\n",
    "        data = {'title': head,\n",
    "                'content': body,\n",
    "                'category': cat}\n",
    "        all_data.append(data)\n",
    "        \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ce37ce82-dd8b-41b2-ad46-fb115f140abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news = []\n",
    "\n",
    "for cat in category:\n",
    "    all_news.append(get_data_inshorts(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ee3fb-5e3f-456a-b370-b50de948cce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
